{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": "## Environment Setup"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": "## Environment Setup Phase\n\n### Step 1: Install Required Packages and Download Model\n\nThis stage completes the following tasks:\n1. **Install LLaMA Model Support Package**: `llama-cpp-python` for running the quantized version of LLaMA 3.1 8B model\n2. **Install Web Search Related Packages**:\n   - `googlesearch-python`: Google Search API\n   - `bs4`: BeautifulSoup web parsing\n   - `charset-normalizer`, `requests-html`, `lxml_html_clean`: Web content processing\n3. **Download Model Weights**: Approximately 8GB quantized model file `Meta-Llama-3.1-8B-Instruct-Q8_0.gguf`\n4. **Download Question Datasets**: `public.txt` and `private.txt` containing questions to be answered\n\n**Note**: Model download requires significant time and sufficient storage space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JywoPOO_Oll"
   },
   "outputs": [],
   "source": "# 安裝LLaMA模型支援套件（支援CUDA 12.2）\n!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n\n# 安裝網路搜尋和網頁解析相關套件\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\n\n# 下載LLaMA 3.1 8B量化模型檔案（約8GB）\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n\n# 下載公開題目資料集\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n\n# 下載私人題目資料集    \nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX6SizAt_Olm"
   },
   "source": "### Step 2: GPU Environment Check\n\nEnsure the runtime environment uses GPU to avoid extremely slow inference speeds. Even the quantized version of LLaMA 3.1 8B model will be very slow on CPU."
  },
  {
   "cell_type": "code",
   "source": "import torch\n\n# 檢查是否正在使用GPU，若否則拋出異常\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": "## Model Loading and Inference Phase\n\n### Step 3: Load LLaMA Model and Create Inference Function\n\nThis stage establishes the core inference capability of the entire system:\n\n1. **Model Loading Configuration**:\n   - `n_gpu_layers=-1`: Load all model layers onto GPU\n   - `n_ctx=16384`: Set context window to 16K tokens (suitable for 16GB VRAM GPU)\n   - `verbose=False`: Disable verbose logging to reduce output\n\n2. **Inference Function Parameter Explanation**:\n   - `max_tokens=512`: Limit generation length to avoid overly long responses\n   - `temperature=0`: Set to 0 for reproducible results, eliminating randomness\n   - `repeat_penalty=2.0`: Prevent model from repeating identical content\n\n**Important**: Context window size directly affects memory usage and needs adjustment based on hardware."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScyW45N__Olm"
   },
   "outputs": [],
   "source": "from llama_cpp import Llama\n\n# 載入LLaMA 3.1 8B模型到GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",  # 模型檔案路徑\n    verbose=False,              # 關閉詳細輸出\n    n_gpu_layers=-1,           # 將所有層載入GPU（-1表示全部）\n    n_ctx=16384,               # 上下文窗口大小：16K tokens，適合16GB VRAM的GPU\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    使用LLaMA模型生成回應的函數\n    \n    參數:\n        _model: LLaMA模型實例\n        _messages: 格式化後的對話訊息\n    \n    返回:\n        str: 模型生成的回應內容\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],  # 停止符號\n        max_tokens=512,          # 最大生成token數量\n        temperature=0,           # 溫度參數：0表示無隨機性，結果可重現\n        repeat_penalty=2.0,      # 重複懲罰：防止模型重複相同內容\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Web Search Tool Phase\n\n### Step 4: Implement Google Search and Web Content Extraction\n\nThis is the **information retrieval core** of the RAG system, responsible for obtaining relevant information from the web:"
  },
  {
   "cell_type": "code",
   "source": "from typing import List\nfrom googlesearch import search as _search\nfrom bs4 import BeautifulSoup\nfrom charset_normalizer import detect\nimport asyncio\nfrom requests_html import AsyncHTMLSession\nimport urllib3\nurllib3.disable_warnings()\n\nasync def worker(s: AsyncHTMLSession, url: str):\n    '''\n    異步獲取單個網頁內容的工作函數\n    \n    參數:\n        s: AsyncHTMLSession實例\n        url: 要抓取的網址\n    \n    返回:\n        str or None: 網頁HTML內容，失敗時返回None\n    '''\n    try:\n        # 先檢查網頁標頭，確認是HTML格式\n        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n            return None\n        \n        # 獲取完整網頁內容\n        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n        return r.text\n    except:\n        return None\n\nasync def get_htmls(urls):\n    '''\n    並行獲取多個網頁的HTML內容\n    \n    參數:\n        urls: 網址列表\n    \n    返回:\n        list: HTML內容列表\n    '''\n    session = AsyncHTMLSession()\n    tasks = (worker(session, url) for url in urls)\n    return await asyncio.gather(*tasks)\n\nasync def search(keyword: str, n_results: int=3) -> List[str]:\n    '''\n    搜尋關鍵字並返回前n個網頁的文字內容\n    \n    參數:\n        keyword: 搜尋關鍵字\n        n_results: 需要返回的結果數量\n    \n    返回:\n        List[str]: 網頁文字內容列表\n    \n    注意: 可能遇到HTTP 429錯誤（Google搜尋頻率限制）\n    '''\n    keyword = keyword[:100]  # 限制關鍵字長度\n    \n    # 獲取搜尋結果（取2倍數量以防部分無效）\n    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n    \n    # 並行獲取網頁HTML內容\n    results = await get_htmls(results)\n    \n    # 過濾掉無效結果\n    results = [x for x in results if x is not None]\n    \n    # 使用BeautifulSoup解析HTML\n    results = [BeautifulSoup(x, 'html.parser') for x in results]\n    \n    # 提取純文字並移除空白，同時過濾非UTF-8編碼\n    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n    \n    # 返回前n個結果\n    return results[:n_results]",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEIRmZl7_Oln"
   },
   "source": "### Step 5: Test Basic Inference Pipeline\n\nBefore building a complex RAG system, test whether the basic LLM inference functionality works properly. This test ensures:\n- Model loads correctly and can perform inference normally\n- Chinese output format meets Traditional Chinese requirements\n- Inference speed is within acceptable range"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 測試基本LLM推理功能\ntest_question=\"請問誰是 Taylor Swift？\"\n\n# 構建對話訊息格式\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # 系統提示\n    {\"role\": \"user\", \"content\": test_question}, # 用戶問題\n]\n\nprint(generate_response(llama3, messages))",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": "## AI Agent Architecture Phase\n\n### Step 6: LLMAgent Class Design Explanation\n\nThis stage establishes the foundational architecture of the **multi-agent collaborative system**. The LLMAgent class is the core component of the entire RAG system:\n\n**Agent Design Philosophy**:\n- **Role Separation**: Each agent handles specific tasks (question understanding, keyword extraction, Q&A, etc.)\n- **Modularity**: Individual agents can be easily replaced or adjusted\n- **Scalability**: Additional specialized agents can be added in the future\n\n**Class Attribute Explanation**:\n- `role_description`: Defines the agent's identity and expertise domain\n- `task_description`: Clearly specifies the specific task the agent needs to complete\n- `llm`: Specifies the language model backend to use\n\n**Inference Method Features**:\n- **Prompt Engineering**: Places role description and task description in system and user prompts respectively\n- **Format Processing**: Ensures input format matches LLaMA's conversation template\n- **Extensibility**: Reserves interface to support other LLM models\n\nThis design allows us to create specialized agents to handle different stages in the RAG process."
  },
  {
   "cell_type": "code",
   "source": "class LLMAgent():\n    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n        '''\n        初始化LLM Agent\n        \n        參數:\n            role_description: Agent的角色描述（誰）\n            task_description: Agent的任務描述（做什麼）\n            llm: 使用的語言模型標識\n        '''\n        self.role_description = role_description    # 角色描述：定義Agent的身份（例：歷史專家、經理等）\n        self.task_description = task_description    # 任務描述：指示Agent應該解決的具體任務\n        self.llm = llm                             # LLM標識：指示Agent使用的語言模型後端\n        \n    def inference(self, message: str) -> str:\n        '''\n        執行推理並返回結果\n        \n        參數:\n            message: 輸入訊息\n            \n        返回:\n            str: Agent的回應\n        '''\n        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF':  # 使用預設模型\n            # 格式化訊息：將角色和任務分別放入system和user prompt\n            messages = [\n                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # 系統角色描述\n                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # 任務描述 + 用戶訊息\n            ]\n            return generate_response(llama3, messages)\n        else:\n            # 如果要使用其他LLM，需要在此實現相應的推理邏輯\n            return \"\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "source": "### Step 7: Design Three Specialized Agents\n\nBased on RAG process requirements, create three agents with distinct responsibilities:\n\n**1. Question Extraction Agent (question_extraction_agent)**\n- **Function**: Extract core questions from complex descriptions\n- **Importance**: Remove interfering information for more precise search\n- **Example**: Simplify \"School songs are representative songs of schools, which school's song is 'Tiger Mountain Heroic Wind Flying'?\" to \"Which school's song is 'Tiger Mountain Heroic Wind Flying'?\"\n\n**2. Keyword Extraction Agent (keyword_extraction_agent)**\n- **Function**: Extract 2-5 most suitable search keywords from questions\n- **Strategy**: Focus on entity nouns, proper nouns, and other concrete searchable terms\n- **Output Format**: Comma-separated keyword list\n\n**3. Q&A Agent (qa_agent)**\n- **Function**: Answer questions based on retrieved data\n- **Role**: Serves as the final knowledge integrator\n- **Output Requirements**: Use Traditional Chinese, answer based on provided context\n\nThis three-stage division design can improve the professionalism and accuracy of each step."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "source": "# 設計三個專門化的Agent來處理RAG流程\n\n# Agent 1: 問題萃取Agent - 負責從複雜描述中提取核心問題\nquestion_extraction_agent = LLMAgent(\n    role_description=\"你是一位專業的問題分析師，擅長從複雜的敘述中找出真正需要解決的問題。你只會用繁體中文回答。\",\n    task_description=\"請從下列敘述中，萃取出最核心、需要解答的問題，並忽略與問題無關的背景或多餘資訊。只需輸出精簡明確的問題句。\",\n)\n\n# Agent 2: 關鍵字萃取Agent - 負責提取適合搜尋的關鍵字\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"你是一位專業的關鍵字萃取專家，擅長從問題中找出最適合用來搜尋的關鍵字。你只會用繁體中文回答。\",\n    task_description=\"請從下列問題中，萃取出最適合用來搜尋的 2~5 個關鍵字或短語。只需輸出關鍵字，並以逗號分隔。\",\n)\n\n# Agent 3: 問答Agent - 負責基於檢索到的資料回答問題\nqa_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n    task_description=\"請回答以下問題：\",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": "## RAG Core Implementation Phase\n\n### Step 8: Install RAG-Related Packages\n\nTo implement Retrieval-Augmented Generation, the following key packages need to be installed:\n\n**Core Package Explanation**:\n- `sentence-transformers`: Pre-trained models for text vectorization\n- `chromadb`: Lightweight vector database supporting similarity search\n- `langchain`: Provides RAG toolchain and embedding wrappers\n- `langchain-community`: Extends LangChain functionality\n\nThese packages will help us:\n1. Convert text into high-dimensional vector representations\n2. Store and quickly retrieve similar documents\n3. Calculate semantic similarity scores"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 安裝RAG所需的額外套件\n\\!pip install sentence-transformers chromadb langchain\n\\!pip install -U langchain-community",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": "### Step 9: Load Multilingual Embedding Model\n\n**Model Selection Rationale**:\n- `paraphrase-multilingual-MiniLM-L12-v2` is specifically designed for multilingual sentence transformers\n- Supports Chinese semantic understanding, suitable for Traditional Chinese questions\n- Moderate model size (~471MB), balancing performance and resource usage\n\n**Embedding Function**:\n- Converts text into 384-dimensional vectors\n- Semantically similar texts have closer distances in vector space\n- Supports cross-lingual semantic search capabilities\n\nThis step lays the foundation for subsequent similarity calculations."
  },
  {
   "cell_type": "code",
   "source": "from sentence_transformers import SentenceTransformer\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\n\n# 載入多語言embedding模型，支援中文語義理解\nembedding_model = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 10: Implement Complete RAG Pipeline\n\nThis is the **core function** of the entire system, integrating all components to complete the end-to-end Q&A process:"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "async def pipeline(question: str) -> str:\n    \"\"\"\n    完整的RAG處理流程\n    \"\"\"\n    # 階段1: 問題理解與預處理\n    core_question = question_extraction_agent.inference(question)\n    keywords = keyword_extraction_agent.inference(core_question)\n    \n    # 階段2: 資訊檢索\n    search_results = await search(keywords, n_results=5)\n    \n    # 階段3: 文檔分割與向量化\n    chunk_size = 500\n    docs = []\n    for doc in search_results:\n        for i in range(0, len(doc), chunk_size):\n            chunk = doc[i:i+chunk_size]\n            if len(chunk) > 50:\n                docs.append(chunk)\n    \n    # 建立Chroma向量資料庫\n    vector_db = Chroma.from_texts(texts=docs, embedding=embedding_model)\n    \n    # 階段4: 相似性搜尋\n    top_k = 5\n    relevant_docs_and_scores = vector_db.similarity_search_with_score(core_question, k=top_k)\n    relevant_docs = [doc[0].page_content for doc in relevant_docs_and_scores]\n    \n    # 摘要\n    summaries = []\n    for chunk in relevant_docs:\n        summary = qa_agent.inference(f\"請將以下資料摘要成100字重點：\\n{chunk}\")\n        summaries.append(summary)\n    context = \"\\n\".join(summaries)\n    \n    # 最終問答\n    final_input = f\"根據以下資料回答問題：\\n{context}\\n問題：{core_question}\"\n    answer = qa_agent.inference(final_input)\n    return answer",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 11: Test RAG Pipeline\n\nUse the 2024 Paris Olympics date as a test case to verify basic RAG system functionality:\n- Test whether search function works properly\n- Check answer generation quality\n- Ensure the entire process runs smoothly"
  },
  {
   "cell_type": "code",
   "source": "# 測試RAG pipeline是否正常運作\nresult = await pipeline(\"請問2024年巴黎奧運的舉辦日期是什麼？請詳細說明。\")\nprint(result)",
   "metadata": {},
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Batch Processing and Result Output Phase\n\n### Step 12: Batch Process All Questions\n\n**Processing Strategy Explanation**:\n- **Resume Mechanism**: Check existing answer files to avoid duplicate processing\n- **Per-Question Saving**: Save each answer immediately to prevent progress loss due to interruption\n- **Memory Management**: Release related resources after processing each question\n\n**File Naming Convention**:\n- Individual answers: `{STUDENT_ID}_{question_number}.txt`\n- Convenient for tracking progress and debugging\n\n**Important Notes**:\n- Colab environment may disconnect due to usage limits\n- Mounting Google Drive ensures persistent file storage\n- Re-execution will automatically skip completed questions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pathlib import Path\n\n# 設定學生ID（需要修改為實際的學生ID）\nSTUDENT_ID = \"20250707\"\n\nSTUDENT_ID = STUDENT_ID.lower()\n\n# 處理public.txt中的題目（前30題）\nwith open('./public.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    questions = [l.strip().split(',')[0] for l in questions]  # 提取問題部分，忽略答案\n    \n    for id, question in enumerate(questions, 1):\n        # 檢查該題是否已經處理過（斷點續傳機制）\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        \n        # 使用RAG pipeline處理問題\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')  # 移除換行符以便後續處理\n        print(id, answer)\n        \n        # 立即保存答案，防止程式中斷時遺失\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)\n\n# 處理private.txt中的題目（後60題）\nwith open('./private.txt', 'r') as input_f:\n    questions = input_f.readlines()\n    for id, question in enumerate(questions, 31):  # 從第31題開始編號\n        # 檢查該題是否已經處理過\n        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n            continue\n        \n        # 使用RAG pipeline處理問題\n        answer = await pipeline(question)\n        answer = answer.replace('\\n',' ')\n        print(id, answer)\n        \n        # 保存答案\n        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n            print(answer, file=output_f)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "source": "### Step 13: Integrate Results and Generate CSV File\n\n**Output Format Explanation**:\n- **CSV Format**: Contains Question (Q) and Answer (A) columns\n- **Encoding Handling**: Use UTF-8 to ensure proper Chinese display\n- **Question Source**: Merge public.txt (first 30 questions) and private.txt (last 60 questions)\n\n**File Purpose**:\n- Convenient result viewing and analysis\n- Meets assignment submission format requirements\n- Can be imported into Excel and other tools for further processing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": "import csv\n\nSTUDENT_ID = \"20250707\"\noutput_csv = f'./{STUDENT_ID}.csv'\n\n# 讀取所有題目（public.txt + private.txt）\nquestions = []\nwith open('./public.txt', 'r', encoding='utf-8') as f:\n    questions += [l.strip().split(',')[0] for l in f.readlines()]  # 只取問題部分\nwith open('./private.txt', 'r', encoding='utf-8') as f:\n    questions += [l.strip().split(',')[0] for l in f.readlines()]\n\n# 將結果寫入CSV檔案\nwith open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['Q', 'A'])  # 寫入標題行\n\n    for idx, question in enumerate(questions, 1):\n        ans_path = f'./{STUDENT_ID}_{idx}.txt'\n        try:\n            # 讀取對應的答案檔案\n            with open(ans_path, 'r', encoding='utf-8') as ans_f:\n                answer = ans_f.readline().strip()\n        except FileNotFoundError:\n            answer = ''  # 如果答案檔不存在，留空\n        writer.writerow([question, answer])",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": "### Step 14: Merge All Answers into Single Text File\n\nCombine all 90 question answers into one text file in order, one answer per line. This format is convenient for:\n- Quick browsing of all answers\n- Batch processing or analysis\n- Use as backup file"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 將所有答案合併成一個文字檔\nwith open(f\"./{\\STUDENT_ID}.txt\", \"w\") as output_f:\n    for id in range(1, 91):\n        with open(f\"./{\\STUDENT_ID}_{id}.txt\", \"r\") as input_f:\n            answer = input_f.readline().strip()\n            print(answer, file=output_f)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 15: Package All Result Files\n\n**Package Contents**:\n- Main CSV result file\n- 90 individual answer files\n\n**Download Functionality**:\n- Automatically generate download links\n- Clean temporary folders to save space"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plUDRTi_B39S"
   },
   "outputs": [],
   "source": "import shutil\nimport os\nfrom IPython.display import FileLink, display\n\nSTUDENT_ID = \"20250707\"\n\n# 1. 指定要打包的檔案清單\nfiles_to_zip = [f\"{STUDENT_ID}.csv\"]  # 主要CSV結果檔\nfiles_to_zip += [f\"{STUDENT_ID}_{i}.txt\" for i in range(1, 91)]  # 90個個別答案檔\n\n# 2. 建立暫存資料夾並複製檔案\ntmp_dir = \"tmp_zip\"\nos.makedirs(tmp_dir, exist_ok=True)\nfor file in files_to_zip:\n    if os.path.exists(file):\n        shutil.copy(file, tmp_dir)\n\n# 3. 壓縮成zip檔案\nzip_name = f\"{STUDENT_ID}_all_answers\"\nshutil.make_archive(zip_name, 'zip', tmp_dir)\n\n# 4. 產生下載連結（適用於Colab環境）\ndisplay(FileLink(f\"{zip_name}.zip\"))\n\n# 5. 清理暫存資料夾以節省空間\nshutil.rmtree(tmp_dir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmLO9PlmEBPn"
   },
   "source": "## System Performance Analysis and Problem Summary\n\n### Main Reasons for Slow Runtime\n\n**1. Multiple LLM Inference Calls**\n- Each question requires 6+ model inferences:\n  - Question Extraction Agent: 1 time\n  - Keyword Extraction Agent: 1 time  \n  - Summary generation: 5 times (for each relevant document)\n  - Final Q&A: 1 time\n- Each inference requires GPU computation, accumulating significant time\n\n**2. Sequential Execution Bottleneck**\n- RAG pipeline stages execute serially, cannot be parallelized\n- Must wait for web search results before proceeding with subsequent processing\n- Vectorization and similarity calculations need to be completed step by step\n\n**3. Network I/O Overhead**\n- Google Search API call latency\n- Network latency from parallel web page scraping\n- HTTP request retry mechanisms increase waiting time\n\n**4. Vector Operation Cost**\n- Document embedding computation (each chunk needs vectorization)\n- Distance calculations for similarity search\n- ChromaDB creation and query operations\n\n### Answer Accuracy Problem Analysis\n\n**Core Issue**: Fundamental reasons why the RAG system cannot accurately answer questions\n\n**1. Keyword Extraction Failure**\n- Example: \"Which school's song is 'Tiger Mountain Heroic Wind Flying'?\"\n- System-extracted keywords may be too broad\n- Causes search results to deviate from question core\n\n**2. Low Search Result Relevance**\n- Google search returns web content that doesn't match questions\n- Particularly for specific, detailed questions\n- Lacks verification mechanism for search result quality\n\n**3. Semantic Similarity Misjudgment**\n- Embedding model may not correctly understand Chinese semantic differences\n- Vector similarity search finds document fragments that aren't truly relevant\n- Fixed 500-character splitting may break semantic integrity\n\n**4. Answer Generation Drift**\n- QA Agent generates answers based on incorrect or irrelevant context\n- Lacks assessment of retrieved content credibility\n- Model tends to answer \"based on data\" even when data is irrelevant\n\n### Implementation Improvement Suggestions\n\n**Short-term Improvements**:\n1. Adjust keyword extraction strategy, add entity recognition\n2. Increase search result relevance filtering\n3. Implement multi-round search mechanism (re-search when first attempt fails)\n4. Improve document segmentation method (semantic boundary cutting)\n\n**Long-term Optimization**:\n1. Use specialized Chinese embedding models\n2. Build question type classification system\n3. Implement answer confidence assessment\n4. Add knowledge graph-assisted retrieval\n\n### System Applicability Assessment\n\n**Suitable Question Types**:\n- General knowledge questions\n- Current event-related queries\n- Latest information requiring web search\n\n**Unsuitable Question Types**:\n- Questions requiring precise answers\n- Local, detailed professional knowledge\n- Mathematical questions requiring reasoning or calculation\n\nThis analysis shows that the current RAG implementation is more suitable as a general Q&A system rather than a precise Q&A tool for specific domains."
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}