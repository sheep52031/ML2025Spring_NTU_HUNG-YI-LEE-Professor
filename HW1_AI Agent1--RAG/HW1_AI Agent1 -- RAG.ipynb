{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": "## Environment Setup Phase\n\n### Step 1: Install Required Packages and Download Model\n\nThis stage completes the following tasks:\n1. **Install LLaMA Model Support Package**: `llama-cpp-python` for running the quantized version of LLaMA 3.1 8B model\n2. **Install Web Search Related Packages**:\n   - `googlesearch-python`: Google Search API\n   - `bs4`: BeautifulSoup web parsing\n   - `charset-normalizer`, `requests-html`, `lxml_html_clean`: Web content processing\n3. **Download Model Weights**: Approximately 8GB quantized model file `Meta-Llama-3.1-8B-Instruct-Q8_0.gguf`\n4. **Download Question Datasets**: `public.txt` and `private.txt` containing questions to be answered\n\n**Note**: Model download requires significant time and sufficient storage space."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JywoPOO_Oll"
   },
   "outputs": [],
   "source": "# 安裝LLaMA模型支援套件（支援CUDA 12.2）\n!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n\n# 安裝網路搜尋和網頁解析相關套件\n!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n\nfrom pathlib import Path\n\n# 下載LLaMA 3.1 8B量化模型檔案（約8GB）\nif not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n\n# 下載公開題目資料集\nif not Path('./public.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n\n# 下載私人題目資料集    \nif not Path('./private.txt').exists():\n    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "kX6SizAt_Olm"
   },
   "outputs": [],
   "source": "### Step 2: GPU Environment Check\n\nEnsure the runtime environment uses GPU to avoid extremely slow inference speeds. Even the quantized version of LLaMA 3.1 8B model will be very slow on CPU."
  },
  {
   "cell_type": "code",
   "source": "import torch\n\n# 檢查是否正在使用GPU，若否則拋出異常\nif not torch.cuda.is_available():\n    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\nelse:\n    print('You are good to go!')",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": "## Model Loading and Inference Phase\n\n### Step 3: Load LLaMA Model and Create Inference Function\n\nThis stage establishes the core inference capability of the entire system:\n\n1. **Model Loading Configuration**:\n   - `n_gpu_layers=-1`: Load all model layers onto GPU\n   - `n_ctx=16384`: Set context window to 16K tokens (suitable for 16GB VRAM GPU)\n   - `verbose=False`: Disable verbose logging to reduce output\n\n2. **Inference Function Parameter Explanation**:\n   - `max_tokens=512`: Limit generation length to avoid overly long responses\n   - `temperature=0`: Set to 0 for reproducible results, eliminating randomness\n   - `repeat_penalty=2.0`: Prevent model from repeating identical content\n\n**Important**: Context window size directly affects memory usage and needs adjustment based on hardware."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtepTeT3_Olm"
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScyW45N__Olm"
   },
   "outputs": [],
   "source": "from llama_cpp import Llama\n\n# 載入LLaMA 3.1 8B模型到GPU\nllama3 = Llama(\n    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",  # 模型檔案路徑\n    verbose=False,              # 關閉詳細輸出\n    n_gpu_layers=-1,           # 將所有層載入GPU（-1表示全部）\n    n_ctx=16384,               # 上下文窗口大小：16K tokens，適合16GB VRAM的GPU\n)\n\ndef generate_response(_model: Llama, _messages: str) -> str:\n    '''\n    使用LLaMA模型生成回應的函數\n    \n    參數:\n        _model: LLaMA模型實例\n        _messages: 格式化後的對話訊息\n    \n    返回:\n        str: 模型生成的回應內容\n    '''\n    _output = _model.create_chat_completion(\n        _messages,\n        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],  # 停止符號\n        max_tokens=512,          # 最大生成token數量\n        temperature=0,           # 溫度參數：0表示無隨機性，結果可重現\n        repeat_penalty=2.0,      # 重複懲罰：防止模型重複相同內容\n    )[\"choices\"][0][\"message\"][\"content\"]\n    return _output"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   },
   "source": "## Web Search Tool Phase\n\n### Step 4: Implement Google Search and Web Content Extraction\n\nThis is the **information retrieval core** of the RAG system, responsible for obtaining relevant information from the web:\n\n**Search Process Explanation**:\n1. **Keyword Processing**: Limit search term length to avoid overly long queries\n2. **Search Strategy**: Obtain 2x result count to prevent invalid web pages\n3. **Asynchronous Processing**: Use `AsyncHTMLSession` to improve multi-page scraping efficiency\n4. **Content Filtering**:\n   - Check Content-Type to ensure HTML format\n   - Set 10-second timeout to avoid hanging on slow websites\n   - Filter non-UTF-8 encoded content to ensure proper Chinese processing\n\n**Limitations and Considerations**:\n- **HTTP 429 Errors**: Google has search frequency limits; excessive use will result in temporary blocking\n- **Uncontrollable Limits**: Google hasn't published specific limitation standards\n- **Solutions**: Reduce search frequency or change IP address\n\nThe quality of this search tool directly affects the accuracy of the RAG system's answers."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "bEIRmZl7_Oln"
   },
   "outputs": [],
   "source": "### Step 5: Test Basic Inference Pipeline\n\nBefore building a complex RAG system, test whether the basic LLM inference functionality works properly. This test ensures:\n- Model loads correctly and can perform inference normally\n- Chinese output format meets Traditional Chinese requirements\n- Inference speed is within acceptable range"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dmGCARd_Oln"
   },
   "outputs": [],
   "source": "# 測試基本LLM推理功能\ntest_question='請問誰是 Taylor Swift？'\n\n# 構建對話訊息格式\nmessages = [\n    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # 系統提示\n    {\"role\": \"user\", \"content\": test_question}, # 用戶問題\n]\n\nprint(generate_response(llama3, messages))"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": "## AI Agent Architecture Phase\n\n### Step 6: LLMAgent Class Design Explanation\n\nThis stage establishes the foundational architecture of the **multi-agent collaborative system**. The LLMAgent class is the core component of the entire RAG system:\n\n**Agent Design Philosophy**:\n- **Role Separation**: Each agent handles specific tasks (question understanding, keyword extraction, Q&A, etc.)\n- **Modularity**: Individual agents can be easily replaced or adjusted\n- **Scalability**: Additional specialized agents can be added in the future\n\n**Class Attribute Explanation**:\n- `role_description`: Defines the agent's identity and expertise domain\n- `task_description`: Clearly specifies the specific task the agent needs to complete\n- `llm`: Specifies the language model backend to use\n\n**Inference Method Features**:\n- **Prompt Engineering**: Places role description and task description in system and user prompts respectively\n- **Format Processing**: Ensures input format matches LLaMA's conversation template\n- **Extensibility**: Reserves interface to support other LLM models\n\nThis design allows us to create specialized agents to handle different stages in the RAG process."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGsIPud3_Oln"
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "outputs": [],
   "source": "### Step 7: Design Three Specialized Agents\n\nBased on RAG process requirements, create three agents with distinct responsibilities:\n\n**1. Question Extraction Agent (question_extraction_agent)**\n- **Function**: Extract core questions from complex descriptions\n- **Importance**: Remove interfering information for more precise search\n- **Example**: Simplify \"School songs are representative songs of schools, which school's song is 'Tiger Mountain Heroic Wind Flying'?\" to \"Which school's song is 'Tiger Mountain Heroic Wind Flying'?\"\n\n**2. Keyword Extraction Agent (keyword_extraction_agent)**\n- **Function**: Extract 2-5 most suitable search keywords from questions\n- **Strategy**: Focus on entity nouns, proper nouns, and other concrete searchable terms\n- **Output Format**: Comma-separated keyword list\n\n**3. Q&A Agent (qa_agent)**\n- **Function**: Answer questions based on retrieved data\n- **Role**: Serves as the final knowledge integrator\n- **Output Requirements**: Use Traditional Chinese, answer based on provided context\n\nThis three-stage division design can improve the professionalism and accuracy of each step."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ueJrgP_Oln"
   },
   "source": [
    "TODO 1: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "source": "# 設計三個專門化的Agent來處理RAG流程\n\n# Agent 1: 問題萃取Agent - 負責從複雜描述中提取核心問題\nquestion_extraction_agent = LLMAgent(\n    role_description=\"你是一位專業的問題分析師，擅長從複雜的敘述中找出真正需要解決的問題。你只會用繁體中文回答。\",\n    task_description=\"請從下列敘述中，萃取出最核心、需要解答的問題，並忽略與問題無關的背景或多餘資訊。只需輸出精簡明確的問題句。\",\n)\n\n# Agent 2: 關鍵字萃取Agent - 負責提取適合搜尋的關鍵字\nkeyword_extraction_agent = LLMAgent(\n    role_description=\"你是一位專業的關鍵字萃取專家，擅長從問題中找出最適合用來搜尋的關鍵字。你只會用繁體中文回答。\",\n    task_description=\"請從下列問題中，萃取出最適合用來搜尋的 2~5 個關鍵字或短語。只需輸出關鍵字，並以逗號分隔。\",\n)\n\n# Agent 3: 問答Agent - 負責基於檢索到的資料回答問題\nqa_agent = LLMAgent(\n    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n    task_description=\"請回答以下問題：\",\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9eoywr7_Oln"
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   },
   "source": [
    "TODO 2: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRGNa-1i_Oln"
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": "## RAG Core Implementation Phase\n\n### Step 8: Install RAG-Related Packages\n\nTo implement Retrieval-Augmented Generation, the following key packages need to be installed:\n\n**Core Package Explanation**:\n- `sentence-transformers`: Pre-trained models for text vectorization\n- `chromadb`: Lightweight vector database supporting similarity search\n- `langchain`: Provides RAG toolchain and embedding wrappers\n- `langchain-community`: Extends LangChain functionality\n\nThese packages will help us:\n1. Convert text into high-dimensional vector representations\n2. Store and quickly retrieve similar documents\n3. Calculate semantic similarity scores"
  },
  {
   "cell_type": "markdown",
   "source": "## RAG核心實作階段\n\n### 第八步：安裝RAG相關套件\n\n為了實現檢索增強生成，需要安裝以下關鍵套件：\n\n**核心套件說明**：\n- `sentence-transformers`：用於文本向量化的預訓練模型\n- `chromadb`：輕量級向量資料庫，支援相似性搜尋\n- `langchain`：提供RAG工具鏈和embedding封裝\n- `langchain-community`：擴展LangChain功能\n\n這些套件將幫助我們：\n1. 將文本轉換為高維向量表示\n2. 儲存和快速檢索相似文檔\n3. 計算語義相似性分數",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": "### Step 9: Load Multilingual Embedding Model\n\n**Model Selection Rationale**:\n- `paraphrase-multilingual-MiniLM-L12-v2` is specifically designed for multilingual sentence transformers\n- Supports Chinese semantic understanding, suitable for Traditional Chinese questions\n- Moderate model size (~471MB), balancing performance and resource usage\n\n**Embedding Function**:\n- Converts text into 384-dimensional vectors\n- Semantically similar texts have closer distances in vector space\n- Supports cross-lingual semantic search capabilities\n\nThis step lays the foundation for subsequent similarity calculations."
  },
  {
   "cell_type": "code",
   "source": "# 安裝RAG所需的額外套件\n!pip install sentence-transformers chromadb langchain\n!pip install -U langchain-community",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 第十步：實作完整RAG Pipeline\n\n這是整個系統的**核心函數**，整合所有組件完成端到端的問答流程：\n\n**RAG流程詳解**：\n\n**階段1：問題理解與預處理**\n- 使用問題萃取Agent移除無關背景資訊\n- 透過關鍵字萃取Agent生成搜尋用關鍵字\n\n**階段2：資訊檢索**\n- 執行Google搜尋獲取相關網頁（預設5筆結果）\n- 將長文檔切分成500字chunks避免上下文溢出\n- 過濾過短文檔片段（少於50字）\n\n**階段3：向量化與相似性搜尋**\n- 建立Chroma向量資料庫儲存所有文檔chunks\n- 計算問題與文檔的cosine相似度\n- 取出最相關的前5個文檔片段\n\n**階段4：內容摘要與答案生成**\n- 對每個相關文檔進行100字摘要，控制輸入長度\n- 將摘要內容作為context提供給QA Agent\n- 生成基於檢索資料的最終答案\n\n**設計考量**：\n- **異步處理**：搜尋操作使用async提升效率\n- **記憶體管理**：透過摘要避免超出模型上下文限制\n- **品質控制**：多階段過濾確保資料品質",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Step 10: Implement Complete RAG Pipeline\n\nThis is the **core function** of the entire system, integrating all components to complete the end-to-end Q&A process:\n\n**RAG Process Detailed Explanation**:\n\n**Stage 1: Question Understanding and Preprocessing**\n- Use Question Extraction Agent to remove irrelevant background information\n- Generate search keywords through Keyword Extraction Agent\n\n**Stage 2: Information Retrieval**\n- Execute Google search to obtain relevant web pages (default 5 results)\n- Split long documents into 500-character chunks to avoid context overflow\n- Filter overly short document fragments (less than 50 characters)\n\n**Stage 3: Vectorization and Similarity Search**\n- Build Chroma vector database to store all document chunks\n- Calculate cosine similarity between questions and documents\n- Extract the most relevant top 5 document fragments\n\n**Stage 4: Content Summarization and Answer Generation**\n- Summarize each relevant document into 100 characters to control input length\n- Provide summarized content as context to QA Agent\n- Generate final answer based on retrieved data\n\n**Design Considerations**:\n- **Asynchronous Processing**: Search operations use async for improved efficiency\n- **Memory Management**: Avoid exceeding model context limits through summarization\n- **Quality Control**: Multi-stage filtering ensures data quality",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 批量處理與結果輸出階段\n\n### 第十二步：批量處理所有題目\n\n**處理策略說明**：\n- **斷點續傳機制**：檢查已存在的答案檔案，避免重複處理\n- **逐題保存**：每題答案立即保存，防止因中斷而遺失進度\n- **記憶體管理**：處理完每題後釋放相關資源\n\n**檔案命名規則**：\n- 個別答案：`{STUDENT_ID}_{題號}.txt`\n- 方便追蹤進度和除錯\n\n**注意事項**：\n- Colab環境可能因使用限制而中斷連線\n- 掛載Google Drive可確保檔案持久保存\n- 重新執行時會自動跳過已完成的題目"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "### Step 11: Test RAG Pipeline\n\nUse the 2024 Paris Olympics date as a test case to verify basic RAG system functionality:\n- Test whether search function works properly\n- Check answer generation quality\n- Ensure the entire process runs smoothly"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # 1. 問題萃取\n",
    "    core_question = question_extraction_agent.inference(question)\n",
    "    \n",
    "    # 2. 關鍵字萃取\n",
    "    keywords = keyword_extraction_agent.inference(core_question)\n",
    "    \n",
    "    # 3. 搜尋網頁\n",
    "    search_results = await search(keywords, n_results=5)  # 多抓幾筆，增加資訊多樣性\n",
    "    \n",
    "    # 4. 將每個搜尋結果切成小段（每500字一段）\n",
    "    chunk_size = 500\n",
    "    docs = []\n",
    "    for doc in search_results:\n",
    "        for i in range(0, len(doc), chunk_size):\n",
    "            chunk = doc[i:i+chunk_size]\n",
    "            if len(chunk) > 50:  # 過短的段落略過\n",
    "                docs.append(chunk)\n",
    "    \n",
    "    # 5. 建立 Chroma 向量資料庫\n",
    "    vector_db = Chroma.from_texts(texts=docs, embedding=embedding_model)\n",
    "    \n",
    "    # 6. 查詢最相關的段落（例如取前5段）\n",
    "    top_k = 5\n",
    "    relevant_docs_and_scores = vector_db.similarity_search_with_score(core_question, k=top_k)\n",
    "    relevant_docs = [doc[0].page_content for doc in relevant_docs_and_scores]\n",
    "    \n",
    "    # 7. 對每段做摘要（可選，讓 context 更精簡）\n",
    "    summaries = []\n",
    "    for chunk in relevant_docs:\n",
    "        summary = qa_agent.inference(f\"請將以下資料摘要成100字重點：\\n{chunk}\")\n",
    "        summaries.append(summary)\n",
    "    context = \"\\n\".join(summaries)\n",
    "    \n",
    "    # 8. 最終問答\n",
    "    final_input = f\"根據以下資料回答問題：\\n{context}\\n問題：{core_question}\"\n",
    "    answer = qa_agent.inference(final_input)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 測試RAG pipeline是否正常運作\nresult = await pipeline(\"請問2024年巴黎奧運的舉辦日期是什麼？請詳細說明。\")\nprint(result)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "## Batch Processing and Result Output Phase\n\n### Step 12: Batch Process All Questions\n\n**Processing Strategy Explanation**:\n- **Resume Mechanism**: Check existing answer files to avoid duplicate processing\n- **Per-Question Saving**: Save each answer immediately to prevent progress loss due to interruption\n- **Memory Management**: Release related resources after processing each question\n\n**File Naming Convention**:\n- Individual answers: `{STUDENT_ID}_{question_number}.txt`\n- Convenient for tracking progress and debugging\n\n**Important Notes**:\n- Colab environment may disconnect due to usage limits\n- Mounting Google Drive ensures persistent file storage\n- Re-execution will automatically skip completed questions",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### 第十五步：打包所有結果檔案\n\n**打包內容**：\n- 主要CSV結果檔案\n- 90個個別答案檔案（方便除錯和檢查）\n\n**下載功能**：\n- 自動生成下載連結\n- 清理暫存資料夾節省空間\n- 適合Colab環境的檔案匯出方式\n\n**檔案組織**：\n- 系統性地管理所有輸出檔案\n- 確保提交時不遺漏任何內容",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "outputs": [],
   "source": "### Step 13: Integrate Results and Generate CSV File\n\n**Output Format Explanation**:\n- **CSV Format**: Contains Question (Q) and Answer (A) columns\n- **Encoding Handling**: Use UTF-8 to ensure proper Chinese display\n- **Question Source**: Merge public.txt (first 30 questions) and private.txt (last 60 questions)\n\n**File Purpose**:\n- Convenient result viewing and analysis\n- Meets assignment submission format requirements\n- Can be imported into Excel and other tools for further processing"
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": "import csv\n\nSTUDENT_ID = \"20250707\"\noutput_csv = f'./{STUDENT_ID}.csv'\n\n# 讀取所有題目（public.txt + private.txt）\nquestions = []\nwith open('./public.txt', 'r', encoding='utf-8') as f:\n    questions += [l.strip().split(',')[0] for l in f.readlines()]  # 只取問題部分\nwith open('./private.txt', 'r', encoding='utf-8') as f:\n    questions += [l.strip().split(',')[0] for l in f.readlines()]\n\n# 將結果寫入CSV檔案\nwith open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n    writer = csv.writer(csvfile)\n    writer.writerow(['Q', 'A'])  # 寫入標題行\n\n    for idx, question in enumerate(questions, 1):\n        ans_path = f'./{STUDENT_ID}_{idx}.txt'\n        try:\n            # 讀取對應的答案檔案\n            with open(ans_path, 'r', encoding='utf-8') as ans_f:\n                answer = ans_f.readline().strip()\n        except FileNotFoundError:\n            answer = ''  # 如果答案檔不存在，留空\n        writer.writerow([question, answer])"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": "### Step 14: Merge All Answers into Single Text File\n\nCombine all 90 question answers into one text file in order, one answer per line. This format is convenient for:\n- Quick browsing of all answers\n- Batch processing or analysis\n- Use as backup file",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plUDRTi_B39S"
   },
   "outputs": [],
   "source": "import shutil\nimport os\nfrom IPython.display import FileLink, display\n\nSTUDENT_ID = \"20250707\"\n\n# 1. 指定要打包的檔案清單\nfiles_to_zip = [f\"{STUDENT_ID}.csv\"]  # 主要CSV結果檔\nfiles_to_zip += [f\"{STUDENT_ID}_{i}.txt\" for i in range(1, 91)]  # 90個個別答案檔\n\n# 2. 建立暫存資料夾並複製檔案\ntmp_dir = \"tmp_zip\"\nos.makedirs(tmp_dir, exist_ok=True)\nfor file in files_to_zip:\n    if os.path.exists(file):\n        shutil.copy(file, tmp_dir)\n\n# 3. 壓縮成zip檔案\nzip_name = f\"{STUDENT_ID}_all_answers\"\nshutil.make_archive(zip_name, 'zip', tmp_dir)\n\n# 4. 產生下載連結（適用於Colab環境）\ndisplay(FileLink(f\"{zip_name}.zip\"))\n\n# 5. 清理暫存資料夾以節省空間\nshutil.rmtree(tmp_dir)"
  },
  {
   "cell_type": "markdown",
   "source": "### Step 15: Package All Result Files\n\n**Package Contents**:\n- Main CSV result file\n- 90 individual answer files (convenient for debugging and checking)\n\n**Download Functionality**:\n- Automatically generate download links\n- Clean temporary folders to save space\n- Suitable file export method for Colab environment\n\n**File Organization**:\n- Systematically manage all output files\n- Ensure no content is missed during submission",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "STUDENT_ID = \"20250707\"\n",
    "output_csv = f'./{STUDENT_ID}.csv'\n",
    "\n",
    "# 讀取 public.txt 和 private.txt 的題目\n",
    "questions = []\n",
    "with open('./public.txt', 'r', encoding='utf-8') as f:\n",
    "    questions += [l.strip().split(',')[0] for l in f.readlines()]\n",
    "with open('./private.txt', 'r', encoding='utf-8') as f:\n",
    "    questions += [l.strip().split(',')[0] for l in f.readlines()]\n",
    "\n",
    "# 寫入 CSV\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Q', 'A'])  # 標題\n",
    "\n",
    "    for idx, question in enumerate(questions, 1):\n",
    "        ans_path = f'./{STUDENT_ID}_{idx}.txt'\n",
    "        try:\n",
    "            with open(ans_path, 'r', encoding='utf-8') as ans_f:\n",
    "                answer = ans_f.readline().strip()\n",
    "        except FileNotFoundError:\n",
    "            answer = ''  # 若該題還沒跑完，答案留空\n",
    "        writer.writerow([question, answer])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "GmLO9PlmEBPn"
   },
   "outputs": [],
   "source": "## System Performance Analysis and Problem Summary\n\n### Main Reasons for Slow Runtime\n\n**1. Multiple LLM Inference Calls**\n- Each question requires 6+ model inferences:\n  - Question Extraction Agent: 1 time\n  - Keyword Extraction Agent: 1 time  \n  - Summary generation: 5 times (for each relevant document)\n  - Final Q&A: 1 time\n- Each inference requires GPU computation, accumulating significant time\n\n**2. Sequential Execution Bottleneck**\n- RAG pipeline stages execute serially, cannot be parallelized\n- Must wait for web search results before proceeding with subsequent processing\n- Vectorization and similarity calculations need to be completed step by step\n\n**3. Network I/O Overhead**\n- Google Search API call latency\n- Network latency from parallel web page scraping\n- HTTP request retry mechanisms increase waiting time\n\n**4. Vector Operation Cost**\n- Document embedding computation (each chunk needs vectorization)\n- Distance calculations for similarity search\n- ChromaDB creation and query operations\n\n### Answer Accuracy Problem Analysis\n\n**Core Issue**: Fundamental reasons why the RAG system cannot accurately answer questions\n\n**1. Keyword Extraction Failure**\n- Example: \"Which school's song is 'Tiger Mountain Heroic Wind Flying'?\"\n- System-extracted keywords may be too broad\n- Causes search results to deviate from question core\n\n**2. Low Search Result Relevance**\n- Google search returns web content that doesn't match questions\n- Particularly for specific, detailed questions\n- Lacks verification mechanism for search result quality\n\n**3. Semantic Similarity Misjudgment**\n- Embedding model may not correctly understand Chinese semantic differences\n- Vector similarity search finds document fragments that aren't truly relevant\n- Fixed 500-character splitting may break semantic integrity\n\n**4. Answer Generation Drift**\n- QA Agent generates answers based on incorrect or irrelevant context\n- Lacks assessment of retrieved content credibility\n- Model tends to answer \"based on data\" even when data is irrelevant\n\n### Implementation Improvement Suggestions\n\n**Short-term Improvements**:\n1. Adjust keyword extraction strategy, add entity recognition\n2. Increase search result relevance filtering\n3. Implement multi-round search mechanism (re-search when first attempt fails)\n4. Improve document segmentation method (semantic boundary cutting)\n\n**Long-term Optimization**:\n1. Use specialized Chinese embedding models\n2. Build question type classification system\n3. Implement answer confidence assessment\n4. Add knowledge graph-assisted retrieval\n\n### System Applicability Assessment\n\n**Suitable Question Types**:\n- General knowledge questions\n- Current event-related queries\n- Latest information requiring web search\n\n**Unsuitable Question Types**:\n- Questions requiring precise answers\n- Local, detailed professional knowledge\n- Mathematical questions requiring reasoning or calculation\n\nThis analysis shows that the current RAG implementation is more suitable as a general Q&A system rather than a precise Q&A tool for specific domains."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "STUDENT_ID = \"20250707\"\n",
    "\n",
    "# 1. 指定要壓縮的檔案清單\n",
    "files_to_zip = [f\"{STUDENT_ID}.csv\"]  # 先加總表\n",
    "files_to_zip += [f\"{STUDENT_ID}_{i}.txt\" for i in range(1, 91)]  # 加入每題答案\n",
    "\n",
    "# 2. 建立一個暫存資料夾，將所有檔案複製進去\n",
    "tmp_dir = \"tmp_zip\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "for file in files_to_zip:\n",
    "    if os.path.exists(file):\n",
    "        shutil.copy(file, tmp_dir)\n",
    "\n",
    "# 3. 壓縮成 zip 檔\n",
    "zip_name = f\"{STUDENT_ID}_all_answers\"\n",
    "shutil.make_archive(zip_name, 'zip', tmp_dir)\n",
    "\n",
    "# 4. 產生下載連結\n",
    "display(FileLink(f\"{zip_name}.zip\"))\n",
    "\n",
    "# 5. 清理暫存資料夾（可選）\n",
    "shutil.rmtree(tmp_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}