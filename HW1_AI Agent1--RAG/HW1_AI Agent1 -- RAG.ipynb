{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TFwaJir_Olj"
   },
   "source": [
    "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6tQHdH2k_Olk"
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mGx000oZ_Oll"
   },
   "source": [
    "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5JywoPOO_Oll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
      "Collecting llama-cpp-python==0.3.4\n",
      "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp310-cp310-linux_x86_64.whl (445.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m148.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (1.26.4)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.3.4) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.3.4) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.3.4) (2024.2.0)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
      "Collecting googlesearch-python\n",
      "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
      "Collecting requests-html\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting lxml_html_clean\n",
      "  Downloading lxml_html_clean-0.4.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (4.12.3)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from googlesearch-python) (2.32.3)\n",
      "Collecting pyquery (from requests-html)\n",
      "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fake-useragent (from requests-html)\n",
      "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting parse (from requests-html)\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting w3lib (from requests-html)\n",
      "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from lxml_html_clean) (5.3.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.6)\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.1.31)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.5.0)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
      "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
      "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->googlesearch-python) (3.10)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyee<12.0.0,>=11.0.0->pyppeteer>=0.0.14->requests-html) (4.12.2)\n",
      "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Downloading lxml_html_clean-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
      "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
      "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 14.1\n",
      "    Uninstalling websockets-14.1:\n",
      "      Successfully uninstalled websockets-14.1\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.3.0\n",
      "    Uninstalling urllib3-2.3.0:\n",
      "      Successfully uninstalled urllib3-2.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 10.4 which is incompatible.\n",
      "tensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.2 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
      "--2025-07-07 13:08:51--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 3.171.171.128, 3.171.171.65, 3.171.171.6, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.171.171.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250707%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250707T130851Z&X-Amz-Expires=3600&X-Amz-Signature=723ab40762d6514ba3605f67e92cc0d978cb345c0bf243668a0289be92e7fba9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1751897331&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTg5NzMzMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=Ht6ZbiWzTLUbOsca-hvECC4QYSZDu-ZB4TCzTXZXPuQfAvCzzymFigZ1wyUXunnQIiTZt-n8EXszoJNU-1X86IUYjYNl4LAyP3SWS4Ei4kP5jrBXskIQBZXRyZCoNV9qBtjPeE3ew4QBfrkBQxZrREZ7E-H7fvrYKBzfEnHm5138QvI47UOKLXLVuLfKGofoFWqXJ5g0n6011UWJ91pxMNTdv5EzZXfG%7EdsPmVjcyzBGSdHw2mKYvNLydsm442RF6lp-8GaDcYZL917FT2l1J1gxWPdXzHuiLJPDqCYg1xecQrBgleOIVbD9ws%7E4Q96FdpHlL2ABIRbqRhIJ49%7E5Nw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
      "--2025-07-07 13:08:51--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250707%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250707T130851Z&X-Amz-Expires=3600&X-Amz-Signature=723ab40762d6514ba3605f67e92cc0d978cb345c0bf243668a0289be92e7fba9&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1751897331&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MTg5NzMzMX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=Ht6ZbiWzTLUbOsca-hvECC4QYSZDu-ZB4TCzTXZXPuQfAvCzzymFigZ1wyUXunnQIiTZt-n8EXszoJNU-1X86IUYjYNl4LAyP3SWS4Ei4kP5jrBXskIQBZXRyZCoNV9qBtjPeE3ew4QBfrkBQxZrREZ7E-H7fvrYKBzfEnHm5138QvI47UOKLXLVuLfKGofoFWqXJ5g0n6011UWJ91pxMNTdv5EzZXfG%7EdsPmVjcyzBGSdHw2mKYvNLydsm442RF6lp-8GaDcYZL917FT2l1J1gxWPdXzHuiLJPDqCYg1xecQrBgleOIVbD9ws%7E4Q96FdpHlL2ABIRbqRhIJ49%7E5Nw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
      "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.4.15, 13.33.4.109, 13.33.4.78, ...\n",
      "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.4.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8540775840 (8.0G)\n",
      "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
      "\n",
      "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G   297MB/s    in 26s     \n",
      "\n",
      "2025-07-07 13:09:17 (314 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
      "\n",
      "--2025-07-07 13:09:17--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4399 (4.3K) [text/plain]\n",
      "Saving to: ‘public.txt’\n",
      "\n",
      "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-07-07 13:09:18 (256 MB/s) - ‘public.txt’ saved [4399/4399]\n",
      "\n",
      "--2025-07-07 13:09:26--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
      "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
      "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 15229 (15K) [text/plain]\n",
      "Saving to: ‘private.txt’\n",
      "\n",
      "private.txt         100%[===================>]  14.87K  79.7KB/s    in 0.2s    \n",
      "\n",
      "2025-07-07 13:09:27 (79.7 KB/s) - ‘private.txt’ saved [15229/15229]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
    "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
    "\n",
    "from pathlib import Path\n",
    "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
    "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
    "if not Path('./public.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
    "if not Path('./private.txt').exists():\n",
    "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kX6SizAt_Olm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are good to go!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "if not torch.cuda.is_available():\n",
    "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
    "else:\n",
    "    print('You are good to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3iyc1qC_Olm"
   },
   "source": [
    "## Prepare the LLM and LLM utility function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T59vxAo2_Olm"
   },
   "source": [
    "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtepTeT3_Olm"
   },
   "source": [
    "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
    "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVil2Vhe_Olm"
   },
   "source": [
    "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ScyW45N__Olm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# Load the model onto GPU\n",
    "llama3 = Llama(\n",
    "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
    "    verbose=False,\n",
    "    n_gpu_layers=-1,\n",
    "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
    ")\n",
    "\n",
    "def generate_response(_model: Llama, _messages: str) -> str:\n",
    "    '''\n",
    "    This function will inference the model with given messages.\n",
    "    '''\n",
    "    _output = _model.create_chat_completion(\n",
    "        _messages,\n",
    "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
    "        max_tokens=512,    # This argument is how many tokens the model can generate.\n",
    "        temperature=0,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
    "        repeat_penalty=2.0,\n",
    "    )[\"choices\"][0][\"message\"][\"content\"]\n",
    "    return _output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tnHLwq-4_Olm"
   },
   "source": [
    "## Search Tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYM-2ZsE_Olm"
   },
   "source": [
    "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bEIRmZl7_Oln"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from googlesearch import search as _search\n",
    "from bs4 import BeautifulSoup\n",
    "from charset_normalizer import detect\n",
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "async def worker(s:AsyncHTMLSession, url:str):\n",
    "    try:\n",
    "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
    "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
    "            return None\n",
    "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
    "        return r.text\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_htmls(urls):\n",
    "    session = AsyncHTMLSession()\n",
    "    tasks = (worker(session, url) for url in urls)\n",
    "    return await asyncio.gather(*tasks)\n",
    "\n",
    "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
    "    '''\n",
    "    This function will search the keyword and return the text content in the first n_results web pages.\n",
    "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
    "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
    "    '''\n",
    "    keyword = keyword[:100]\n",
    "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
    "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
    "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
    "    results = await get_htmls(results)\n",
    "    # Filter out the None values.\n",
    "    results = [x for x in results if x is not None]\n",
    "    # Parse the HTML.\n",
    "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
    "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
    "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
    "    # Return the first n results.\n",
    "    return results[:n_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3zQjjj_Oln"
   },
   "source": [
    "## Test the LLM inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8dmGCARd_Oln"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
      "\n",
      "泰勒絲早期在鄉郊小鎮演唱會時開始發展音樂事業，她推出了多張專輯，包括《Taylor Swift》、《Fearless》，以及後來更為知名的大熱作如 《1989》（2014年）、_reputation（）和 _Lover （）。她的歌曲經常探討愛情、友誼及自我成長等主題。\n",
      "\n",
      "泰勒絲獲得了許多獎項，包括13座格萊美奖，並且是史上最快達到百萬銷量的女藝人之一。\n"
     ]
    }
   ],
   "source": [
    "# You can try out different questions here.\n",
    "test_question='請問誰是 Taylor Swift？'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
    "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
    "]\n",
    "\n",
    "print(generate_response(llama3, messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0-ojJuE_Oln"
   },
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGsIPud3_Oln"
   },
   "source": [
    "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
    "- Attributes:\n",
    "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
    "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
    "    - llm: Just an indicator of the LLM model used by the agent.\n",
    "- Method:\n",
    "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zjG-UwDX_Oln"
   },
   "outputs": [],
   "source": [
    "class LLMAgent():\n",
    "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
    "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
    "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
    "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
    "    def inference(self, message:str) -> str:\n",
    "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
    "            # TODO: Design the system prompt and user prompt here.\n",
    "            # Format the messsages first.\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
    "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
    "            ]\n",
    "            return generate_response(llama3, messages)\n",
    "        else:\n",
    "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-ueJrgP_Oln"
   },
   "source": [
    "TODO 1: Design the role description and task description for each agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DzPzmNnj_Oln"
   },
   "outputs": [],
   "source": [
    "# TODO: Design the role and task description for each agent.\n",
    "\n",
    "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
    "question_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是一位專業的問題分析師，擅長從複雜的敘述中找出真正需要解決的問題。你只會用繁體中文回答。\",\n",
    "    task_description=\"請從下列敘述中，萃取出最核心、需要解答的問題，並忽略與問題無關的背景或多餘資訊。只需輸出精簡明確的問題句。\",\n",
    ")\n",
    "\n",
    "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
    "keyword_extraction_agent = LLMAgent(\n",
    "    role_description=\"你是一位專業的關鍵字萃取專家，擅長從問題中找出最適合用來搜尋的關鍵字。你只會用繁體中文回答。\",\n",
    "    task_description=\"請從下列問題中，萃取出最適合用來搜尋的 2~5 個關鍵字或短語。只需輸出關鍵字，並以逗號分隔。\",\n",
    ")\n",
    "\n",
    "# This agent is the core component that answers the question.\n",
    "qa_agent = LLMAgent(\n",
    "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
    "    task_description=\"請回答以下問題：\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9eoywr7_Oln"
   },
   "source": [
    "## RAG pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HDOjNYJ_Oln"
   },
   "source": [
    "TODO 2: Implement the RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRGNa-1i_Oln"
   },
   "source": [
    "Please refer to the homework description slides for hints.\n",
    "\n",
    "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMaIsKAZ_Olo"
   },
   "source": [
    "- Naive approach (simple baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mppO-oOO_Olo"
   },
   "source": [
    "- Naive RAG approach (medium baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYxbciLO_Olo"
   },
   "source": [
    "- RAG with agents (strong baseline)\n",
    "\n",
    "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n",
      "Requirement already satisfied: chromadb in /usr/local/lib/python3.10/dist-packages (1.0.15)\n",
      "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.47.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.11.0a1)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.4.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.22.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.34.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.13.0)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.68.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.1)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (33.1.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.10/dist-packages (from chromadb) (3.10.12)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.28.1)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.23.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.11.11)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
      "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from build>=1.0.3->chromadb) (2.2.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.9.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=4.19.0->chromadb) (0.22.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.26.20)\n",
      "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.22.5->chromadb) (2.4.1)\n",
      "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9->chromadb) (2.28.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (10.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (3.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.2)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.22.5->chromadb) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.22.5->chromadb) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.22.5->chromadb) (2024.2.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting langchain-community\n",
      "  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-core<1.0.0,>=0.3.66 (from langchain-community)\n",
      "  Downloading langchain_core-0.3.68-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting langchain<1.0.0,>=0.3.26 (from langchain-community)\n",
      "  Downloading langchain-0.3.26-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.36)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.11.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
      "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.3)\n",
      "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
      "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
      "Collecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.26->langchain-community)\n",
      "  Downloading langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.0a1)\n",
      "Collecting langsmith>=0.1.125 (from langchain-community)\n",
      "  Downloading langsmith-0.4.4-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
      "Collecting zstandard<0.24.0,>=0.23.0 (from langsmith>=0.1.125->langchain-community)\n",
      "  Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.2->langchain-community) (2.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.28.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.28.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.2->langchain-community) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.2->langchain-community) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.26.2->langchain-community) (2024.2.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.2.2)\n",
      "Downloading langchain_community-0.3.27-py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
      "Downloading langchain-0.3.26-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.68-py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langsmith-0.4.4-py3-none-any.whl (367 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Downloading zstandard-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: zstandard, typing-inspection, httpx-sse, pydantic-settings, langsmith, langchain-core, langchain-text-splitters, langchain, langchain-community\n",
      "  Attempting uninstall: langsmith\n",
      "    Found existing installation: langsmith 0.2.3\n",
      "    Uninstalling langsmith-0.2.3:\n",
      "      Successfully uninstalled langsmith-0.2.3\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.25\n",
      "    Uninstalling langchain-core-0.3.25:\n",
      "      Successfully uninstalled langchain-core-0.3.25\n",
      "  Attempting uninstall: langchain-text-splitters\n",
      "    Found existing installation: langchain-text-splitters 0.3.3\n",
      "    Uninstalling langchain-text-splitters-0.3.3:\n",
      "      Successfully uninstalled langchain-text-splitters-0.3.3\n",
      "  Attempting uninstall: langchain\n",
      "    Found existing installation: langchain 0.3.12\n",
      "    Uninstalling langchain-0.3.12:\n",
      "      Successfully uninstalled langchain-0.3.12\n",
      "Successfully installed httpx-sse-0.4.1 langchain-0.3.26 langchain-community-0.3.27 langchain-core-0.3.68 langchain-text-splitters-0.3.8 langsmith-0.4.4 pydantic-settings-2.10.1 typing-inspection-0.4.1 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers chromadb langchain\n",
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-deba1e031b18>:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f05eb52c0c45ec8d4a31aa6e1b37ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a8de4131d343a3aa8b6de33ec893de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56ba22a48644a28ab97276cf8c1b1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58b98a16b7847cebdd1f072ff07e928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62d2f0a0d1c46508f2cf33b75b20ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1388e384d72b45e7ad10e9c8077b848f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25cecd403974634b7eff3bd0ab2bf24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598c5808b1f846ba834fc1a20a327ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc3a2ccbac44973ada3b40a24e5d7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ad7de51962c4c7cbbdcf50f2e7eafd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1. 載入 embedding 模型\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def pipeline(question: str) -> str:\n",
    "    # 1. 問題萃取\n",
    "    core_question = question_extraction_agent.inference(question)\n",
    "    \n",
    "    # 2. 關鍵字萃取\n",
    "    keywords = keyword_extraction_agent.inference(core_question)\n",
    "    \n",
    "    # 3. 搜尋網頁\n",
    "    search_results = await search(keywords, n_results=5)  # 多抓幾筆，增加資訊多樣性\n",
    "    \n",
    "    # 4. 將每個搜尋結果切成小段（每500字一段）\n",
    "    chunk_size = 500\n",
    "    docs = []\n",
    "    for doc in search_results:\n",
    "        for i in range(0, len(doc), chunk_size):\n",
    "            chunk = doc[i:i+chunk_size]\n",
    "            if len(chunk) > 50:  # 過短的段落略過\n",
    "                docs.append(chunk)\n",
    "    \n",
    "    # 5. 建立 Chroma 向量資料庫\n",
    "    vector_db = Chroma.from_texts(texts=docs, embedding=embedding_model)\n",
    "    \n",
    "    # 6. 查詢最相關的段落（例如取前5段）\n",
    "    top_k = 5\n",
    "    relevant_docs_and_scores = vector_db.similarity_search_with_score(core_question, k=top_k)\n",
    "    relevant_docs = [doc[0].page_content for doc in relevant_docs_and_scores]\n",
    "    \n",
    "    # 7. 對每段做摘要（可選，讓 context 更精簡）\n",
    "    summaries = []\n",
    "    for chunk in relevant_docs:\n",
    "        summary = qa_agent.inference(f\"請將以下資料摘要成100字重點：\\n{chunk}\")\n",
    "        summaries.append(summary)\n",
    "    context = \"\\n\".join(summaries)\n",
    "    \n",
    "    # 8. 最終問答\n",
    "    final_input = f\"根據以下資料回答問題：\\n{context}\\n問題：{core_question}\"\n",
    "    answer = qa_agent.inference(final_input)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ztJkA7R7_Olo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根據資料，2024年巴黎奧運會的開幕式日期是7月26日，而閉門時間則是在8 月11 日。\n"
     ]
    }
   ],
   "source": [
    "result = await pipeline(\"請問2024年巴黎奧運的舉辦日期是什麼？請詳細說明。\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_kI_9EGB0S9"
   },
   "source": [
    "## Answer the questions using your pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN17sSZ8DUg7"
   },
   "source": [
    "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "plUDRTi_B39S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 根據資料，我們無法直接找到「虎山雄風飛揚」是哪間學校的校歌。然而，文中提到大同市第七中学啦拉操舞動青春熱情似火彰顯當代精神，這可能與你要找的是一樣東西\n",
      "2 根據資料，NCC透過行政命令規定民眾境外郵購自用產品如無線鍑盤、滑鼠等，每案加收審查費750元。\n",
      "3 根據資料，第一代 iPhone 的發表者是史蒂夫·乔布斯（Steve Jobs）。他在2007年1月9日推出iPhone時展現了其多功能性，並展示了一系列的特點，如音樂播放、視頻觀看、一鍵拨打電話和发送短信/電子郢等。\n",
      "4 根據資料，部分美國大學需滿分90，而阿爾伯塔省加拿大研究所需要達到86。因此，如果你想申請進階英文免修，你應該至少要在托福網路測驗 TOEFL iBT 中取得 89 分以上的成績，以便能夠符合這些大學或研究生的入學要求。但是，建議參照各校官 網公告為準。\n",
      "5 根據資料，觸地時間的目標是讓前腳掌落在身體正下方。\n",
      "6 根據資料，卑南族的祖先發源地在巴拿馬。\n",
      "7 根據資料，史蒂夫·乔布斯在里德学院学习期间的指導教授是罗伯特・弗理达兰（Robert Friedland）。\n",
      "8 根據資料，詹姆斯·克拉 克麦克斯韦（James Clerk Maxwell）是發現電磁感應定律的人。\n",
      "9 根據資料，距離國立臺灣史前文化博物館最近的臺鐵車站為康樂站在台南。\n",
      "10 根據整數加法的性質，當 a 和 b 都是正数 時，其和為：a+b=|а+б|=20 + 30 = |50|  因此答案結果等於:| а |- б |=: **60**\n",
      "11 根據資料，達拉斯獨行俠隊的Luka Doncic被交易至湖人（Los Angeles Lakers）球队。\n",
      "12 根據 Associated Press 的報導，2024 年美國總統選舉結果如下：  *   民主党：喬拜登和凱米哈里斯获得总统候选人资格     *共和黨:唐納德川普再次成为総裁競爭對手\n",
      "13 根據資料，Llama-3.2 系列模型中參數量最小的為 1B。\n",
      "14 根據資料，學生每個学期可以停修不超過3門課程，但特殊情況可豁免限制。\n",
      "15 根據資料，DeepSeek是一家中國公司，但沒有提到它的母 公司是誰。\n",
      "16 根據資料，2024年NBA總冠軍隊伍是波士顿塞尔提克（Boston Celtics），但在你提供的新資訊中顯示了另一支球队奪得  Nba 總決賽。\n",
      "17 根據資料，鏈烚可分為飽和與不饱合。其中，不餓含有單鍵、雙键或叄锭的碳连接。  因此，我們可以推斷三級（Containing three bonds）是指具有三个单双 键 或 叁 鍊 的 碲連結，例如 CH₃CH₂CHOH 是一種 三级烷類化合物\n",
      "18 根據資料，查爾斯·巴貝奇被認為是計算機科學之父，因他設計了分析机（現代電腦前身），並探索數字穿孔卡上的指令進行任何数学運算。\n",
      "19 根據資料，臺灣玄天上帝信仰的進香中心位於台北市。\n",
      "20 根據資料，Windows 作業系統是由微軟公司開發的。\n",
      "21 根據資料，官將首的起源於新莊地藏庵。\n",
      "22 根據資料，莎布·尼古拉丝（Shub-Niggurath）是一位克苏鲁神话中的邪女，被稱為黑暗丰穰之母。\n",
      "23 根據提供的資料，我們無法直接找到答案，但可以透過一些推測和分析來回答。  「短暫交會旅程就此分岔」這句歌詞似乎與遊戲故事有關，特別是皇帝前往冰海遇到的情節。然而，這不是一個明確的問題，因此我將試圖提供可能答案：  根據我的知識庫，我無法找到任何相關信息或證据指出「短暫交會旅程就此分岔」這句歌詞屬於哪個團體或者個人。但是，基礎上可以推測它很有機率與遊戲的主題、情節和角色故事線相連。  如果你能提供更多資訊或背景，我將更樂意幫助您找出答案。\n",
      "24 根據資料，我們可以看到卑南族聯合年聚的主辦部落在不同年的變化。然而，沒有明確提到2025 年會舉行於哪個特定的利嘉國小或是其他地點。  但是在提供給你的文本中，並未有任何一份資料指出 2月1日（初四）卑南族聯合年祭將在何處主辦。\n",
      "25 根據資料，最新的輝達顯卡系列是「GeForce RTX 50」、「RT40」，以及 「30」。\n",
      "26 根據資料，我們知道史蒂夫·乔布斯（Steve Jobs）因胰腺癌逝世紀念 (2011)，但沒有提到他是在哪個國家旅遊時去世。\n",
      "27 根據資料，牛頓是發明了萬有引力的物理學家。他通過運用自己的三大運動定律和慣性原理推導出万有的吸附公式：F=Gm1*m2/r^２。\n",
      "28 根據資料，我們無法找到台鵠開示計畫「TAIHUCAIS」的英文全名。\n",
      "29 根據資料，「I'll be back」是《終結者》電影的經典台詞。\n",
      "30 根據資料，水的化學式為H2O。\n",
      "31 很抱歉，我們無法在資料中找到李宏毅教授的《機器學習》課程內容。\n",
      "32 根據資料，我們可以知道該獨立學院是浙江財經大學東風学院。\n",
      "33 根據 BitTorrent 協議规范，新節點可以通過以下方式從其他種子隨機獲得部分資料：  1.  **DHT (分布散列表)**：Bit Torrent 的 DHA 功能允許 Peer 間直接溝通，而不需要 Tracker介入。這樣一來，每個Peer 都能夠與其它的Peers 直接交換資訊，從而實現了資料分佈和查找。 2.  **Tracker**：雖然 DHT 可以讓 Peer 間直接溝通，但在某些情況下還是需要 Tracker 的介入。例如，在新節點加入時，它可能會向一個或多個Trackers 查詢，得到最近的資料分佈資訊。 3.  **種子文件 (Seed File)**：當原始檔案被分享到網路上後，一些Peer 可能成為「超級种子的节点」，這意味著它們擁有完整版本或大部分內容。新節點可以通過連接至此類型的 Peer，從而獲得資料。 4.  **磁力鏈結 (Magnet Link)**：Bit Torrent 的 Magnet Links 可以直接下載 BT 文件，而不需要任何 Tracker 或種子文件。  總之，這些機制都有助於新節點能夠隨意地與其他 Peer 溝通，從而獲得所需的資料。\n",
      "34 該影片是科幻動作電影《終結者》。\n",
      "35 根據資料，戈芬氏鳳頭鸚鵡偏好的乳酪口味是藍莓。\n",
      "36 根據資料，桃園Xpark的企鵝寶貝名字是「Tomorin」。\n",
      "37 根據資料，國立臺灣大學物理治療學系的課程簡介中提到四年制六個組別選擇。\n",
      "38 根據資料，「呼嘿」笑聲習慣的角色是大和麻弥。\n",
      "39 根據資料，甲斐之虎是影狼的別名。\n",
      "40 根據提供的資料，我們無法確定王肥貓同學最有可能去修哪一門課。然而，從洋碧美語托福考試相關資訊中可以看出，他很想提高自己的英語能力並通過高分 托菲測驗。  因此，最合理的情況是他會選擇一些與英文或口說技巧有關的學習計畫，或許是在浙江財經大學東方學院院級課題成果中提到的獨立學校自主管理相關科目。\n",
      "41 根據提供的資料，我們無法找到2024年的第42回《極限體能王SASUKE》首播日期。然而，節目通常在每年12月底或1初左右播放新的一季，所以你可以試著查詢相關資訊，或等待官方公布新的消息。  另外，我們知道前幾屆的比賽時間如下：  * 第38回（SASUKE2020）：於 20 年度第4期播出，約為四小時。   如果這個模式持續，那麼你可以試著推測下一季可能會在哪一個月份播放。\n",
      "42 根據資料，我們可以知道馬智禮出生於卑南族，但後來成為初鹿頭目的是漢人。\n",
      "43 根據資料，BanGDream!AveMujica 的OP主題歌為「KiLL KiSS」由 Ave Mujic 表演。\n",
      "44 根據資料，Linux作業系統的首次發布年份是1991 年。\n",
      "45 根據資料，卑南族 Likavung 部落的中文名稱為利嘉部落。\n",
      "46 根據資料，紅茶其實是全發酵的類型，而不是中間或介於不完全未經過任何氧氣和普洱之中的。\n",
      "47 根據資料，真紅眼黑龍與「巴斯達布雷达」作為融合素材的怪獸是《破壞劍士》。\n",
      "48 根據資料，豐田萌繪在《BanG Dream!》中擔任三角初華/Doloris（聲：佐々木李子）的角色。\n",
      "49 根據資料，9 號球員的正式名稱是 \"中锋\" 或者簡稱為 ９號。\n",
      "50 根據資料，天王星是被降格成矮行 星的那顆 天體。\n",
      "51 根據資料，臺灣最早成立的野生動物救傷單位是位於該地區內，但具體行政區域未被提及。\n",
      "52 根據資料，特有生物研究保育中心在1999年配合精省作業改隸行政院農委會，更名為「特殊種植物園」或是更正答案： 特殊种植园\n",
      "53 根據資料，提出的模型名字是 \"Formosa １モデル（ F11 -３Ｂ）\"。\n",
      "54 根據資料，太陽系中體積最大的行星是木衛（Jupiter），其大小約為1300000倍地球的尺寸。\n",
      "55 根據資料，魯凱語系被認定為最早分化的南島群體之一。\n",
      "56 根據提供的資料，我們可以看到槽香澄是一位不善表達自己想法的人，但在外人面前會維持一個良好的形象。然而，這個問題其實不是問誰是老師，而是在詢问為何沒有分組。  因此，答案應該是不明確，因爲資料中並未提及任何有關學生是否需要進行小班教導或什麼樣的情況下會被安排到同一群體內的資訊。\n",
      "57 根據資料，排灣原住民族的打招呼用語有許多種類，但並沒有提到「embiyax namu kana」是哪一臺灣原始族群。\n",
      "58 根據提供的資料，我們可以知道：  * 鄒與布農，永久美麗」這句話是卑南族聯合年聚活動主題「narunirulr臀鈴呼喚」的口號。  因此，這個問題答案為：鄄 卞 侖\n",
      "59 根據資料，角色小明（役角）為女主後裔鬼神鎮「咒法堂」現任主人。\n",
      "60 根據資料，Tuku 創建了射馬幹部落。\n",
      "61 根據資料，長野誠以總合第1名（99分）完全制霸，因此《終極一班》中的「KO榜」第一位是 長 野 誠。\n",
      "62 根據提供的資料，Linux核心中的CFS（Complete Fair Scheduling）使用紅黑樹來儲存排程相關資訊。這是因為在 Linux 預設 CPU 排行中需要頻繁插入和移除節點，而 AVL 樹會導致性能較差，因此選擇了更適合的红-black 树實現。  CFS 使用紅黑樹來儲存排程相關資訊，包括進程序列、優先級等信息。這樣可以確保查找插入和刪除操作在最壞情況下時間複雜度均達 O(logn) 且能夠高效地管理 CPU 排行。  紅黑樹的使用也使得 Linux 核心中的 CFS 能够更好 地支持多個進程共享CPU資源，提高系統整體性能。\n",
      "63 不正確。根據資料，霸王行動（Operation Overlord）是一場盟軍登陸西北歐的大規模戰役，但作為這次大规模战略计划中的突擊階段，被稱为海洋星行动。  然而，在問題中提到諾曼第的「奧運會」（ OperationOverLord），其實是霸王行動（Operation Overlord）的別名。\n",
      "64 根據資料，我們無法直接找到《Cytus II》遊戲中「Body Talk」是哪位角色的歌曲的資訊。然而，黃鴻升曾經創作過多首單張，如<鬼混>、金刚变形等，但這些信息與題目關聯不大。  但根據《終極一班電視原聲帶》的資料，我們可以發現有個叫做「Body Talk」的歌曲。\n",
      "65 根據資料，我們可以知道李琳山教授的演講被意大利工程師LuigiMenabrea轉錄成法語後，被艾达洛夫莱斯翻譯為英文，並在论文中添加了注釋。\n",
      "66 根據資料，RTX 5090 顯卡的 VRAM 設計為32GB GDDR7。\n",
      "67 根據資料，2024年世界棒球12強賽冠軍為台灣隊。\n",
      "68 根據資料，中國四大奇書包括《水滸傳》、《西遊記》，以及三國演義和金瓶梅。\n",
      "69 根據資料，子時是指晚上11:00至01：30。\n",
      "70 根據資料，避免要錯過時限來完成作業的排程演算法稱為CFS（完全公平调度）。\n",
      "71 根據資料，C8763之父出現啦！黑雪姬三招心意技能名稱與桐人的相同：\"Vorpalstrike\"\"Starbuststream \"\"Theeclipse \"\n",
      "72 根據資料，「統領坡」位於今日的屏東縣。\n",
      "73 根據提供的資料，Google Colab 的訂閱制中，不需要任何計畫即可使用 A100 高級 GPU。\n",
      "74 根據提供的資料，李宏毅老師開設了多個課程，但沒有明確指出哪一門屬於機器學習。然而，我們可以推測他可能是電腦科學院或資訊工程系的一員，因為他的教材和內容都與這些領域有關。  根據提供的資料，李宏毅老師開設了以下課程：  *   《深度学习实践指南》：是一本优化并详细推导公式、重点讲解难点知识适合读者轻松入门。     *  教材选取部分来自他的公开课，并补充了一些除这堂课程之外的相关知識。此书由上海交通大学博士生QiWang等人贡献编写，是一本深度学习实践指南，适合初学者和想要加强基础知识的人。 *   動手學 深 度 學習 v1：線性回歸 和 基礎優化算法     *  台大李宏毅教授的強 化 課程  因此，我們可以推測他可能是電腦科學院或資訊工程系的一員。\n",
      "75 根據資料，學士班若修習学分数低于最小应补充的学习单位，则需填写申请减免。\n",
      "76 根據提供的資料，我們無法直接找到 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 哪個角色。\n",
      "77 資料中沒有提到誰劫持了愛蜜莉雅並想取其為妻。\n",
      "78 根據資料，海綿寶宝在第五季《失蹤記》中擊敗刺破泡沫紅眼幇是在白鷺城。\n",
      "79 根據資料，玉米其實是單子葉植物。\n",
      "80 根據資料，我們可以知道中華民國陸軍的歌曲前六字是：  「我武維揚後備」\n",
      "81 根據所提供的資料，台大理學院成員包括物理、化學和數碼等科目。\n",
      "82 根據資料，我們可以看到以下五個月球地形位於不面對地球的背後：  1.  水神星（Mercury） 2\\. 金王子 (Venus) 3.\\ 地 球( Earth ) 4\\ .火 神 星(Mars)  5 \\。木天土海阋妊\n",
      "83 《C♯小調第14號鋼琴奏鳴曲》又稱「月光」。\n",
      "84 根據資料，阿米斯音樂節是由舒MIEN．魯碧創立的。\n",
      "85 根據資料，我們無法找到關於 Poppy Playtime - Chapter 4 中的黏土人名字相關資訊。\n",
      "86 根據資料，賓茂村位於金崙溪上游北側，並沒有明確提到它屬於哪一行政區劃。然而，我們可以從台湾主要山脉分为西部和东 部的描述中推測，它可能位于東臺灣地區內，但具體所在縣市則無法確認。  但根據資料中的另一段，賓茂村位於排灣族舊社附近，並提到該區域有警察官吏駐所以及原住民族佔絕大多數。這使我們可以推測它可能位于台東地區內，但具體所在縣市則無法確認。  如果你能提供更多資訊或上下文，我們就更容易幫助您找出賓茂村的行政區劃了！\n",
      "87 根據資料，米開朗基洛《大衛》雕像最初是在佛羅倫斯領主廣場創作的。\n",
      "88 根據資料，除了蔣中正之外，我們無法從提供的資訊得知曾短暫晉升特級上將的人是誰。\n",
      "89 根據資料，台北暗殺星（TPA）贏得了2012年「英雄聯盟」世界大賽的冠軍。\n",
      "90 根據日本麻將的規則，非莊家一開始的手牌有14張。\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Fill in your student ID first.\n",
    "STUDENT_ID = \"20250707\"\n",
    "\n",
    "STUDENT_ID = STUDENT_ID.lower()\n",
    "with open('./public.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    questions = [l.strip().split(',')[0] for l in questions]\n",
    "    for id, question in enumerate(questions, 1):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
    "            print(answer, file=output_f)\n",
    "\n",
    "with open('./private.txt', 'r') as input_f:\n",
    "    questions = input_f.readlines()\n",
    "    for id, question in enumerate(questions, 31):\n",
    "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
    "            continue\n",
    "        answer = await pipeline(question)\n",
    "        answer = answer.replace('\\n',' ')\n",
    "        print(id, answer)\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "STUDENT_ID = \"20250707\"\n",
    "output_csv = f'./{STUDENT_ID}.csv'\n",
    "\n",
    "# 讀取 public.txt 和 private.txt 的題目\n",
    "questions = []\n",
    "with open('./public.txt', 'r', encoding='utf-8') as f:\n",
    "    questions += [l.strip().split(',')[0] for l in f.readlines()]\n",
    "with open('./private.txt', 'r', encoding='utf-8') as f:\n",
    "    questions += [l.strip().split(',')[0] for l in f.readlines()]\n",
    "\n",
    "# 寫入 CSV\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Q', 'A'])  # 標題\n",
    "\n",
    "    for idx, question in enumerate(questions, 1):\n",
    "        ans_path = f'./{STUDENT_ID}_{idx}.txt'\n",
    "        try:\n",
    "            with open(ans_path, 'r', encoding='utf-8') as ans_f:\n",
    "                answer = ans_f.readline().strip()\n",
    "        except FileNotFoundError:\n",
    "            answer = ''  # 若該題還沒跑完，答案留空\n",
    "        writer.writerow([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "GmLO9PlmEBPn"
   },
   "outputs": [],
   "source": [
    "# Combine the results into one file.\n",
    "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
    "    for id in range(1,91):\n",
    "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
    "            answer = input_f.readline().strip()\n",
    "            print(answer, file=output_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "STUDENT_ID = \"20250707\"\n",
    "\n",
    "# 1. 指定要壓縮的檔案清單\n",
    "files_to_zip = [f\"{STUDENT_ID}.csv\"]  # 先加總表\n",
    "files_to_zip += [f\"{STUDENT_ID}_{i}.txt\" for i in range(1, 91)]  # 加入每題答案\n",
    "\n",
    "# 2. 建立一個暫存資料夾，將所有檔案複製進去\n",
    "tmp_dir = \"tmp_zip\"\n",
    "os.makedirs(tmp_dir, exist_ok=True)\n",
    "for file in files_to_zip:\n",
    "    if os.path.exists(file):\n",
    "        shutil.copy(file, tmp_dir)\n",
    "\n",
    "# 3. 壓縮成 zip 檔\n",
    "zip_name = f\"{STUDENT_ID}_all_answers\"\n",
    "shutil.make_archive(zip_name, 'zip', tmp_dir)\n",
    "\n",
    "# 4. 產生下載連結\n",
    "display(FileLink(f\"{zip_name}.zip\"))\n",
    "\n",
    "# 5. 清理暫存資料夾（可選）\n",
    "shutil.rmtree(tmp_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
